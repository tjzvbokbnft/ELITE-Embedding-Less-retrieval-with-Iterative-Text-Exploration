#!/usr/bin/env python
# coding: utf-8
"""
cli_agent.py â€” ç»ˆç«¯ç›´æ¥è·‘çš„ç‰ˆæœ¬
ç”¨æ³•ç¤ºä¾‹ï¼ˆè¦†ç›–éƒ¨åˆ†å‚æ•°ï¼‰ï¼š
    python cli_agent.py \
        --novel nvQA/Frankenstein.txt \
        --recall_index 8 \
        --neighbor_num 20 \
        --common_model gpt-4o

ä¸ä¼ ä»»ä½•å‚æ•°å³å…¨éƒ¨èµ°é»˜è®¤å€¼ã€‚
"""

import argparse
import time
from pathlib import Path

from src import core_functions, utils          # ä½ çš„ä¸šåŠ¡ä»£ç 
import local_config as cfg                     # ç»Ÿä¸€åªè¿™ä¹ˆå¯¼å…¥


# ---------- CLI å‚æ•° ----------
def parse_cli() -> dict:
    p = argparse.ArgumentParser(
        description="Run the Q&A agent over a novel with optional hyper-parameters."
    )
    # é€šç”¨è¶…å‚
    p.add_argument("--recall_index",      type=int, help=f"default={cfg.recall_index}")
    p.add_argument("--neighbor_num",      type=int, help=f"default={cfg.neighbor_num}")
    p.add_argument("--deep_search_index", type=int, help=f"default={cfg.deep_search_index}")
    p.add_argument("--deep_search_num",   type=int, help=f"default={cfg.deep_search_num}")
    p.add_argument("--voter_num",         type=int, help=f"default={cfg.voter_num}")
    p.add_argument("--num_ctx",           type=int, help=f"default={cfg.num_ctx}")
    p.add_argument("--common_model",      type=str, help=f"default='{cfg.common_model}'")

    # å°è¯´æ–‡ä»¶
    p.add_argument(
        "--novel",
        type=str,
        help="Path to novel text file (default=nvQA/Frankenstein.txt).",
        default="nvQA/Frankenstein.txt",
    )

    # True/False æ¥å£ï¼Œä¸¾ä¾‹
    # p.add_argument("--dry_run", action="store_true", help="Don't call LLM, debug only")

    args = p.parse_args()

    # æŠŠ None çš„å­—æ®µå‰”æ‰ï¼Œåªè¦†ç›–ç”¨æˆ·æ˜¾å¼ä¼ å…¥çš„
    overrides = {k: v for k, v in vars(args).items() if v is not None}
    return overrides


# ---------- ä¸»é€»è¾‘ ----------
def _write_logs(round_idx, log_dir, query, answer, ctx_retrieval, mem_retrieval, new_keywords):
    def _append(path: Path, header: str, body: str):
        with path.open("a", encoding="utf-8") as f:
            f.write(f"\n\n==== Round {round_idx} ====\n")
            f.write(f"ğŸ§â€â™‚ï¸ User: {query}\n")
            f.write(f"{header}\n{body.strip()}\n")

    _append(log_dir / "agent_mem.txt", "ğŸ§  Agent Memory (new):", answer)
    _append(log_dir / "context_retrieval_log.txt", "ğŸ¤– Context retrieval:", ctx_retrieval)
    _append(log_dir / "mem_retrieval_log.txt", "ğŸ¤– Memory retrieval:", mem_retrieval)
    _append(log_dir / "keywords_cached.txt", "ğŸ¤– Keywords cached:", new_keywords)


def main():
    # 1. è§£æ CLI å¹¶è¦†ç›–é…ç½®
    cli_kwargs = parse_cli()
    novel_path = Path(cli_kwargs.pop("novel", "nvQA/Frankenstein.txt"))
    cfg.set_config(**cli_kwargs)  # åªæ›´æ–°ä¼ å…¥çš„é¡¹

    # 2. è¯»å–å°è¯´
    if not novel_path.is_file():
        raise FileNotFoundError(f"âŒ Novel file not found: {novel_path}")
    context = novel_path.read_text(encoding="utf-8")
    print(f"ğŸ§  å·²åŠ è½½æ–‡æœ¬: {novel_path}")
    print("ğŸ’¬ è¾“å…¥é—®é¢˜ï¼ˆexit é€€å‡ºï¼‰")

    # 3. å†·å¯åŠ¨
    cold_start = [
        "What core themes and genres best describe this novel?",
        "What is the central narrative hook or premise introduced early in the story?",
        "How would you describe the author's writing style and tone, based on the opening chapters?",
    ]

    cache_keywords = ""
    memory = ""
    timestamp = int(time.time())
    log_dir = Path(f"DEMO_LOG/{cfg.common_model}+{timestamp}")
    log_dir.mkdir(parents=True, exist_ok=True)
    round_idx = 0

    # 4. ä¸»å¾ªç¯
    while True:
        query = (
            cold_start[round_idx] if round_idx < len(cold_start)
            else input("You: ").strip()
        )
        if query.lower() in {"exit", "quit"}:
            print("ğŸ‘‹ Exiting.")
            break

        mem_retrieval = ""
        new_keywords = ""

        # Memory æ£€ç´¢
        if memory:
            resp_mem = core_functions.retrieve_useful(
                text_input=memory,
                query=query,
                cached_keywords=cache_keywords,
            )
            mem_retrieval = resp_mem["retrieve_data"]
            new_keywords += resp_mem["keywords_extracted"]

        # Context æ£€ç´¢
        resp_ctx = core_functions.retrieve_useful(
            text_input=context,
            query=query,
            cached_keywords=cache_keywords,
        )
        ctx_retrieval = resp_ctx["retrieve_data"]
        new_keywords += resp_ctx["keywords_extracted"]

        # è°ƒ LLM
        final_input = (
            f"RETRIEVAL FROM CONTEXT:\n{ctx_retrieval}\n\n"
            f"RETRIEVAL FROM MEMORY:\n{mem_retrieval}\n\n"
            f"BASED on the retrievals above, respond to the user's query: {query}"
        )
        answer = core_functions.send(final_input)
        memory += answer

        # å…³é”®è¯æŠ½å–
        mem_keywords = core_functions.send(
            "Context: "
            f"{query}\n"
            "PROMPT: Extract proper keywords/element/entity/location from given context "
            "Output the keywords **ONLY** in the following format: "
            "[\"keyword1\", \"keyword2\", \"keyword3\"]"
        )
        new_keywords += mem_keywords
        cache_keywords += new_keywords

        # è¾“å‡º & æ—¥å¿—
        print(f"\nğŸ¤– Agent:\n{answer.strip()}\n{'=' * 20}")
        _write_logs(
            round_idx, log_dir, query, answer, ctx_retrieval, mem_retrieval, new_keywords
        )
        round_idx += 1


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Exiting by user interruption.")
